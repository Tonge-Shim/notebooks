{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de133df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from googletrans import Translator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af1c212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4030"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = Translator()\n",
    "json_list = os.listdir('./결과물/json_results')\n",
    "len(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "72163ff1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_json = []\n",
    "for filename in json_list:\n",
    "    with open('./결과물/json_results/'+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_json.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0fc30188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english name, path dictionary, write as json file\n",
    "namepath = {}\n",
    "for whole in total_json:\n",
    "    l = list(whole['개체명'].split(','))\n",
    "    ans = []\n",
    "    for x in l:\n",
    "        if x[0] == ' ':\n",
    "            x = x[1:]\n",
    "        ans.append(enkordict[x].lower())\n",
    "    namepath[whole['name']] = ans\n",
    "namepath\n",
    "\n",
    "\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(\"./namepath.json\", \"w\", encoding='utf-8') as outfile:\n",
    "    json.dump(namepath, outfile, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "344cacf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'꽃'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_json[9]['개체명']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd892c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진 띄우기, 옆에 결과 적어놓기, 원래 json파일에 있는 데이터도 적어놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d236752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "object_list = []\n",
    "for json_dict in total_json:\n",
    "    object_list += list(json_dict['개체명'].split(','))\n",
    "object_set = set(object_list)\n",
    "object_set = list(object_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41e16f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(object_set):\n",
    "    if x[0] == ' ':\n",
    "        object_set[i] = x[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cbbb2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "enkordict = {}\n",
    "for kor in object_set:\n",
    "    enkordict[kor] = trans.translate(kor, dest='en').text\n",
    "enkordict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "250c2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i.lower() for i in enkordict.values()]\n",
    "clip_labels = [f\"an illust of a {label}\" for label in labels]\n",
    "clip_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "be1b69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot illusts\n",
    "%matplotlib inline\n",
    "# fig = plt.figure(figsize=(10, 10))\n",
    "for i, label_dict in enumerate(total_json[:100]):\n",
    "    if i % 4 == 0:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "    path = './결과물/masking_tape/'+label_dict['name']\n",
    "    img = Image.open(path)\n",
    "#     img = cv2.imread(path)\n",
    "#     fig.add_subplot(rows, columns, i+1)\n",
    "    fig.add_subplot(1, 4, i%4+1)\n",
    "    eng_label = ''\n",
    "    l = list(label_dict['개체명'].split(','))\n",
    "    for x in l:\n",
    "        if x[0] == ' ':\n",
    "            x = x[1:]\n",
    "        eng_label += enkordict[x] + '/'\n",
    "    plt.title(eng_label)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f1a2a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_QuantizedWeights.DEFAULT\n",
    "model = resnet50(weights=weights, quantize=True)\n",
    "model.eval()\n",
    "\n",
    "# img = read_image(path)\n",
    "for i, label_dict in enumerate(total_json[:10]):\n",
    "    path = './결과물/masking_tape/'+label_dict['name']\n",
    "    img = torch.tensor(cv2.imread(path))\n",
    "    img = img.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "    # Step 2: Initialize the inference transforms\n",
    "    preprocess = weights.transforms()\n",
    "\n",
    "    # Step 3: Apply inference preprocessing transforms\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    # Step 4: Use the model and print the predicted category\n",
    "    prediction = model(batch).squeeze(0).softmax(0)\n",
    "    class_id = prediction.argmax().item()\n",
    "    \n",
    "    score = prediction[class_id].item()\n",
    "    ans = ''\n",
    "    ind = np.argpartition(prediction.detach().numpy(), -4)[-4:]\n",
    "    for x in ind:\n",
    "        ans += weights.meta[\"categories\"][x] + ' '\n",
    "    category_name = weights.meta[\"categories\"][class_id]\n",
    "    print(f\"{category_name}: {100 * score:.1f}%\")\n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d0238a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import json\n",
    "# model_id = 'openai/clip-vit-base-patch32'\n",
    "base_path32 = 'openai/clip-vit-base-patch32'\n",
    "large_patch14 = \"openai/clip-vit-large-patch14\"\n",
    "large_patch14336 = \"openai/clip-vit-large-patch14-336\"\n",
    "\n",
    "model_id = large_patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "15567625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1943e3523eb74da3ac3876f89c506cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755663b54e2f4984bc7e428afdcd9530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71c956b06ba4af48fface8cd5543dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cd643db98042548a025056f2097c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834b0af420c94eba86bdd6db8b18c11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f093b4f3dcf84677843cfb1268742c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530d9a1054c43bc89e27ef476ef5d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40de89149ab744dcb243429ede5340c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize processor, label_tokens, model, device, normalize label embeddings\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "label_tokens = processor(text=clip_labels,\n",
    "                        padding=True,\n",
    "                        images=None,\n",
    "                        return_tensors='pt'\n",
    "                        ).to(device)\n",
    "\n",
    "label_emb = model.get_text_features(**label_tokens)\n",
    "label_emb = label_emb.detach().cpu().numpy()\n",
    "label_emb.shape\n",
    "\n",
    "#normalize\n",
    "label_emb = label_emb / np.linalg.norm(label_emb, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ef1b395f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load image, calculate similarity, retrieve top 3->5 items\n",
    "# print with original labels\n",
    "\n",
    "# with open('./namepath.json', 'r') as f:\n",
    "#     namepath = json.load(f)\n",
    "\n",
    "pred_list = []; label_list = []\n",
    "for i, x in enumerate(namepath):\n",
    "#     if i % 4 == 0:\n",
    "#         fig = plt.figure(figsize=(10, 10))\n",
    "    path = './결과물/masking_tape/' + x\n",
    "    img = Image.open(path)\n",
    "    image = processor(text=None,\n",
    "                     images=img,\n",
    "                     return_tensors='pt'\n",
    "                     )['pixel_values'].to(device)\n",
    "    img_emb = model.get_image_features(image)\n",
    "    img_emb = img_emb.detach().cpu().numpy()\n",
    "    scores = np.dot(img_emb, label_emb.T)\n",
    "    pred = np.argsort(scores[0])[::-1][:5]\n",
    "    top_5 = [labels[x] for x in pred]\n",
    "#     fig.add_subplot(1, 4, i%4+1)\n",
    "    pred_out = 'predicted:'+','.join(top_5)\n",
    "    if 'card' in pred_out or 'cell phone screen' in pred_out:\n",
    "        pred_out += ', polaroid frame'\n",
    "    label = 'label:' + ','.join(namepath[x])\n",
    "    pred_list.append(pred_out)\n",
    "#     label_list.append(label)\n",
    "#     label_list.append(namepath[x])\n",
    "    print(label)\n",
    "    print(pred_out)\n",
    "    print()\n",
    "#     print()\n",
    "#     plt.title(label+'\\n'+pred_out)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d188e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_patch32:  56.55086848635236\n"
     ]
    }
   ],
   "source": [
    "# # base_patch32\n",
    "# cor = 0\n",
    "# total = len(label_list)\n",
    "\n",
    "# for i, x in enumerate(namepath):\n",
    "#     for item in namepath[x]:\n",
    "#         if item in pred_list[i]:\n",
    "#             cor += 1\n",
    "#             break\n",
    "# acc = cor/total*100\n",
    "# print(\"base_patch32: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large-patch14\n",
    "cor = 0\n",
    "total = len(label_list)\n",
    "\n",
    "for i, x in enumerate(namepath):\n",
    "    for item in namepath[x]:\n",
    "        if item in pred_list[i]:\n",
    "            cor += 1\n",
    "            break\n",
    "acc = cor/total*100\n",
    "print(\"large_patch14:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large-patch14-336\n",
    "cor = 0\n",
    "total = len(label_list)\n",
    "\n",
    "for i, x in enumerate(namepath):\n",
    "    for item in namepath[x]:\n",
    "        if item in pred_list[i]:\n",
    "            cor += 1\n",
    "            break\n",
    "acc = cor/total*100\n",
    "print(\"large_patch14_336:\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
